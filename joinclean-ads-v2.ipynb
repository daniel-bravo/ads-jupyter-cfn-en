{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f81ef35",
   "metadata": {},
   "source": [
    "# Notebook to process AWS Application Discovery Output\n",
    "\n",
    "## 1. Parameter definition\n",
    "\n",
    "This notebook performs joining and data processing of data provided by the AWS ADS service. We join the following files:\n",
    "\n",
    "1. EC2 Instance Recommendations\n",
    "2. System Performance\n",
    "\n",
    "The insights this Notebook provides are:\n",
    "\n",
    "1. EC2 cost estimation\n",
    "2. EBS storage estimation\n",
    "3. DTO cost estimation, assuming that 20% of the written traffic goes to the internet (Data Transfer Out - DTO)\n",
    "\n",
    "Additionally, this Notebook provides the following CSV files:\n",
    "\n",
    "1. Recommended instance types, with EBS data, DTO, RAM, IOPs and vCPUs for each instance\n",
    "2. VMs that couldn't be processed by AWS ADS\n",
    "3. EC2 instances with no activity (0 disk IOPs)\n",
    "4. Instances with less than 10 disk IOPs\n",
    "5. Top ten instances, in terms of cost\n",
    "\n",
    "Please modify following variables as per your file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96a45f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# Importing needed libraries\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "# Please replace with the name of the files obtained from AWS ADS\n",
    "ec2_recommendation_file = \"EC2InstanceRecommendations.csv\"\n",
    "system_performance_file = \"SystemPerformance.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88f6ab",
   "metadata": {},
   "source": [
    "In this section we query the EBS prices using Pricing API ( source: https://aws.amazon.com/ebs/pricing ). Please change the region as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac657eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "print(\"Get EBS Volume Prices\")\n",
    "print(\"==============================\")\n",
    "\n",
    "pricing = boto3.client('pricing')\n",
    "\n",
    "#Change 'location' below if needed:\n",
    "response = pricing.get_products(\n",
    "     ServiceCode='AmazonEC2',\n",
    "     Filters = [\n",
    "         {'Type' :'TERM_MATCH', 'Field':'volumeApiName', 'Value':'standard'              }\n",
    "         ,{'Type' :'TERM_MATCH', 'Field':'locationType',  'Value':'AWS Region'                   }\n",
    "         ,{'Type' :'TERM_MATCH', 'Field':'location',      'Value':'US East (N. Virginia)'              }\n",
    "     ],\n",
    "     MaxResults=100\n",
    ")\n",
    "\n",
    "\n",
    "# From the whole response get the list of key names (product codes) that we are intereste on,\n",
    "# which are the OnDemand Price Dimensions. Each range of price has a unique code name that we\n",
    "# don't know in advance. That's why we have to get the keys list to iterate them later.\n",
    "product = json.loads(response['PriceList'][0])\n",
    "ondem = product['terms']['OnDemand']\n",
    "ondem_list = list(ondem.keys())\n",
    "prices = ondem[ondem_list[0]]['priceDimensions']\n",
    "prices_list = list(prices.keys())\n",
    "\n",
    "# Based on the top GBs of the range determine the cost variable of interest\n",
    "for range_code in prices_list:\n",
    "    range = prices[range_code] \n",
    "    if range['endRange'] == 'Inf':\n",
    "        ebs_magnetic_monthly_cost  = float(range['pricePerUnit']['USD'])\n",
    "\n",
    "print('ebs_magnetic_monthly_cost =  $ {} USD'.format(ebs_magnetic_monthly_cost))\n",
    "\n",
    "##########################################\n",
    "\n",
    "#Change 'location' below if needed:\n",
    "response = pricing.get_products(\n",
    "     ServiceCode='AmazonEC2',\n",
    "     Filters = [\n",
    "         {'Type' :'TERM_MATCH', 'Field':'volumeApiName', 'Value':'gp3'              }\n",
    "         ,{'Type' :'TERM_MATCH', 'Field':'locationType',  'Value':'AWS Region'                   }\n",
    "         ,{'Type' :'TERM_MATCH', 'Field':'location',      'Value':'US East (N. Virginia)'              }\n",
    "     ],\n",
    "     MaxResults=100\n",
    ")\n",
    "\n",
    "# From the whole response we search for the specific \"usagetype\" we are looking for between all prices.\n",
    "# From thta product we get the list of key names (product codes) that we are intereste on,\n",
    "# which are the OnDemand Price Dimensions. Each range of price has a unique code name that we\n",
    "# don't know in advance. That's why we have to get the keys list to iterate them later.\n",
    "pricelist = response['PriceList']\n",
    "for i in pricelist:\n",
    "    tmp_prod = json.loads(i)\n",
    "    if tmp_prod['product']['attributes']['usagetype'] == 'EBS:VolumeUsage.gp3':\n",
    "        product = tmp_prod\n",
    "\n",
    "ondem = product['terms']['OnDemand']\n",
    "ondem_list = list(ondem.keys())\n",
    "prices = ondem[ondem_list[0]]['priceDimensions']\n",
    "prices_list = list(prices.keys())\n",
    "\n",
    "# Based on the top GBs of the range determine the cost variable of interest\n",
    "for range_code in prices_list:\n",
    "    range = prices[range_code]\n",
    "    if range['endRange'] == 'Inf':\n",
    "        ebs_gp3_monthly_cost   = float(range['pricePerUnit']['USD'])\n",
    "\n",
    "print('ebs_gp3_monthly_cost  = $ {} USD'.format(ebs_gp3_monthly_cost ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e365b",
   "metadata": {},
   "source": [
    "In this section we query the DTO prices using Pricing API ( source: https://aws.amazon.com/ec2/pricing/on-demand/ ). Please change the region as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6538dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "print(\"Get DataTransfer Prices\")\n",
    "print(\"==============================\")\n",
    "\n",
    "# Change 'location' below if needed:\n",
    "response = pricing.get_products(\n",
    "     ServiceCode='AWSDataTransfer',\n",
    "     Filters = [\n",
    "         {'Type' :'TERM_MATCH', 'Field':'fromLocationType', 'Value':'AWS Region'              }\n",
    "         ,{'Type' :'TERM_MATCH', 'Field':'transferType',  'Value':'AWS Outbound'                   }\n",
    "         ,{'Type' :'TERM_MATCH', 'Field':'fromLocation',      'Value':'US East (N. Virginia)'              }\n",
    "     ],\n",
    "     MaxResults=100\n",
    ")\n",
    "\n",
    "# From the whole response get the list of key names (product codes) that we are intereste on,\n",
    "# which are the OnDemand Price Dimensions. Each range of price has a unique code name that we\n",
    "# don't know in advance. That's why we have to get the keys list to iterate them later.\n",
    "product = json.loads(response['PriceList'][0])\n",
    "ondem = product['terms']['OnDemand']\n",
    "ondem_list = list(ondem.keys())\n",
    "prices = ondem[ondem_list[0]]['priceDimensions']\n",
    "prices_list = list(prices.keys())\n",
    "\n",
    "# Based on the top GBs of the range determine the cost variable of interest\n",
    "for range_code in prices_list:\n",
    "    range = prices[range_code]\n",
    "    \n",
    "    if range['endRange'] == '10240':\n",
    "        cost_10_tb = float(range['pricePerUnit']['USD'])\n",
    "    elif range['endRange'] == '51200':\n",
    "        cost_40_tb = float(range['pricePerUnit']['USD'])\n",
    "    elif range['endRange'] == '153600':\n",
    "        cost_100_tb = float(range['pricePerUnit']['USD'])\n",
    "    elif range['endRange'] == 'Inf':\n",
    "        cost_150_tb = float(range['pricePerUnit']['USD'])\n",
    "\n",
    "print('cost_10_tb = $ {} USD'.format(cost_10_tb))\n",
    "print('cost_40_tb = $ {} USD'.format(cost_40_tb))\n",
    "print('cost_100_tb = $ {} USD'.format(cost_100_tb))\n",
    "print('cost_150_tb = $ {} USD'.format(cost_150_tb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e70ef8",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc7f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Importing the recommendations file\n",
    "ds_rec = pd.read_csv(ec2_recommendation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69581b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Importing the performance file\n",
    "ds_sys = pd.read_csv(system_performance_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ef044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Copying to a dataframe with the needed columns from recommendations dataframe\n",
    "ds_rec_fil = ds_rec[['ServerId','Server.HostName','Server.VMware.VMname','Server.OS.Name','Recommendation.EC2.Instance.OSType','Server.VMware.vCenterName','Recommendation.EC2.Instance.vCPUCount','Recommendation.EC2.Instance.RAM.TotalSizeinMB','Recommendation.EC2.Instance.Model','Recommendation.EC2.Instance.Price.HourlyRate','Server.DiskReadOpsPerSecond.Max','Server.DiskWriteOpsPerSecond.Max']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40042c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# Renaming the cells to something easier to identify\n",
    "ds_rec_fil.rename(columns={'ServerId': 'Server_ID','Server.HostName': 'Server','Server.VMware.VMname':'Vmware_Name','Server.OS.Name':'Operating_System','Recommendation.EC2.Instance.OSType':'OS_Type','Server.VMware.vCenterName':'vCenter_Name','Recommendation.EC2.Instance.vCPUCount':'vCPUs_Recommendation','Recommendation.EC2.Instance.RAM.TotalSizeinMB':'Memory_(GiB)_Recommendation','Recommendation.EC2.Instance.Model':'Recommended_EC2_Instance','Recommendation.EC2.Instance.Price.HourlyRate':'EC2_Hourly_Rate','Server.DiskReadOpsPerSecond.Max':'DiskReadOpsPerSecondMax','Server.DiskWriteOpsPerSecond.Max':'DiskWriteOpsPerSecondMax'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9c817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Showing 10 random rows\n",
    "np.random.seed(1)\n",
    "ds_rec_fil.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c4632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "# Copying to a dataframe with the needed columns from performance dataframe\n",
    "ds_sys_fil = ds_sys[['serverId','numDisks','powerState','totalDiskSize','avgNetworkBytesWrittenPerSecond']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13288529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Renaming the cells to something easier to identify\n",
    "ds_sys_fil.rename(columns={'serverId': 'Server_ID', 'numDisks': 'Num_Disks', 'powerState': 'VM_State(ON/OFF)','totalDiskSize':'Storage_Capacity(GiB)', 'avgNetworkBytesWrittenPerSecond': 'Data_Transfer_Out-GiB_written'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d76eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "# Showing 10 random rows\n",
    "ds_sys_fil.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c305f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "# Showing the data type for each column in the recommendations dataframe\n",
    "ds_rec_fil.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb97a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "# Showing the data type for each column in the performance dataframe\n",
    "ds_sys_fil.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57850cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "# Joining both data frames (inner join)\n",
    "df_merged = ds_rec_fil.merge(ds_sys_fil, on='Server_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b311c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "# Showing 10 random rows\n",
    "df_merged.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da264d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "# Functions to transform MiB to GiB and Bytes to GiB\n",
    "def mb2gb(mb):\n",
    "    return mb/1024\n",
    "\n",
    "def b2gb(b):\n",
    "    if np.isnan(b) == False:\n",
    "        return round(b/1024/1024/1024)\n",
    "\n",
    "# Function to compute the DTO consumed during a month, from the bits written to the network\n",
    "def monthly(b):\n",
    "    if np.isnan(b) == False:\n",
    "        return round(b*3600*24*30.4)\n",
    "\n",
    "# Function to transform bits to GiB\n",
    "def bi2gb(b):\n",
    "    if np.isnan(b) == False:\n",
    "        return round(b/8/1024/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330cf24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "# Transforming Memory data to GiB\n",
    "df_merged['Memory_(GiB)_Recommendation'] = df_merged['Memory_(GiB)_Recommendation'].apply(mb2gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35281f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "# Transforming Memory data to GiB\n",
    "df_merged['Data_Transfer_Out-GiB_written'] = df_merged['Data_Transfer_Out-GiB_written'].apply(monthly)\n",
    "df_merged['Data_Transfer_Out-GiB_written'] = df_merged['Data_Transfer_Out-GiB_written'].apply(b2gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10042a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 19\n",
    "# Transforming EBS storage data to GiB\n",
    "df_merged['Storage_Capacity(GiB)'] = df_merged['Storage_Capacity(GiB)'].apply(b2gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886666ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "# Showing 10 random rows\n",
    "df_merged.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492ead3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 21\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cabaac",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "* If we have more `Server_ID`, w.r.t. `Recommended_EC2_Instance`, it's possible that AWS ADS was not able collect data because the VM was turned off.\n",
    "* We will filter out turned off VMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1068e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "# Selecting turned on VMs only\n",
    "df_on = df_merged[df_merged['VM_State(ON/OFF)'] == 'POWER_ON']\n",
    "df_on.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8841f5a",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "* If event after running previous code, we see that the `Server_ID` non-null > `Recommended_EC2_Instance` non-null, we will need to copy those VMs for which ADS does not have a recommendation.\n",
    "* We have observed it's normal to have more `Server_ID` than `Recommended_EC2_Instance`, mainly because:\n",
    "\n",
    "1. ESX servers\n",
    "2. VMware appliances that we don't need to migrate\n",
    "3. The AWS ADS Connector appliance or any other AWS appliances, such as AWS Storage Gateway\n",
    "4. Instances that don't have VMware Tools installed.\n",
    "\n",
    "We export all VMs falling into \"Non_Recommandation\" case, for its manual treatment, using other tools like, for example, RVTools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb63c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 23\n",
    "# Getting instances without recommendation\n",
    "df_no_recom = df_on[df_on['Recommended_EC2_Instance'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a040dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 24\n",
    "# Saving the instances without recommendation in a CSV file for its further analysis, outside the Notebook\n",
    "df_no_recom.to_csv(\"EC2_Non_Recommandation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abfe5c",
   "metadata": {},
   "source": [
    "### Removing Null values\n",
    "\n",
    "We will select only values for which ADS has a recommendation, prior Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5094a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 25\n",
    "# Counting the rows and columnds\n",
    "df_on_rec = df_on[df_on['Recommended_EC2_Instance'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f61aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 26\n",
    "df_on_rec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea1d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 27\n",
    "# Adding write and read IOPs\n",
    "df_on_rec['Iops_total_(R+W)-Max'] = df_on_rec['DiskReadOpsPerSecondMax'] + df_on_rec['DiskWriteOpsPerSecondMax']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b242e3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 28\n",
    "df_on_rec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda13524",
   "metadata": {},
   "source": [
    "### Optimizing EBS volume types\n",
    "\n",
    "We assume that if the IOPs number is below 100, it's possible to use a previous generation volume in our application: https://aws.amazon.com/ebs/previous-generation/\n",
    "\n",
    "However, it's needed to validate case by case if our application is going to work correctly with a magnetic volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de787927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 29\n",
    "# Functions to determine the EBS type and its price, taking into account the IOPs and the disk size. If disk size > 1024 GiB, they need to be GP3 at least\n",
    "\n",
    "def ebs_type_iops(n):\n",
    "    if n > 100:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ebs_type_gb(n):\n",
    "    if n > 1024:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ebs_type_encode(n):\n",
    "    if n >= 1:\n",
    "        return 'gp3'\n",
    "    else:\n",
    "        return 'magnetic'\n",
    "\n",
    "def ebs_price_encode(n):\n",
    "    if n >= 1:\n",
    "        return ebs_gp3_monthly_cost\n",
    "    else:\n",
    "        return ebs_magnetic_monthly_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38830587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 30\n",
    "df_on_rec['EBS_Type_Iops'] = df_on_rec['Iops_total_(R+W)-Max'].apply(ebs_type_iops)\n",
    "df_on_rec['EBS_Type_Gb'] = df_on_rec['Storage_Capacity(GiB)'].apply(ebs_type_gb)\n",
    "df_on_rec['EBS_Type_Gb+Iops'] = df_on_rec['EBS_Type_Iops'] + df_on_rec['EBS_Type_Gb']\n",
    "df_on_rec['EBS_Type'] = df_on_rec['EBS_Type_Gb+Iops'].apply(ebs_type_encode)\n",
    "df_on_rec['EBS_Price'] = df_on_rec['EBS_Type_Gb+Iops'].apply(ebs_price_encode)\n",
    "# Computing the monthly cost per volume\n",
    "df_on_rec['EBS_Monthly_Cost'] = df_on_rec['EBS_Price'] * df_on_rec['Storage_Capacity(GiB)']\n",
    "# Computing the monthly cost per instance per month, from the hourly rate from each instance\n",
    "df_on_rec['EC2_Monthly_Cost'] = df_on_rec['EC2_Hourly_Rate'] * 730\n",
    "df_on_rec.drop(['EBS_Type_Iops', 'EBS_Type_Gb', 'EBS_Type_Gb+Iops'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ef70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 31\n",
    "df_on_rec.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8dc73",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "With collected data, we draw some graphs that will let us analysing visually the type of recommended instances, the operating systems and the Windows versions distribution.\n",
    "\n",
    "Additionally, we select the list of instances with less than 10 IOPs, which can help us identify low or null utilization VMs and determine if we need to migrate them to AWS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefaa14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 32\n",
    "# Function to graph interesting columns\n",
    "def chart_category(data_column, width, height, add_percentage=False):\n",
    "    plt.figure(figsize=(width,height))\n",
    "    chart = sns.countplot(x=data_column, order = data_column.value_counts().index)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(),rotation=45)\n",
    "    \n",
    "    for p in chart.patches:\n",
    "        total = len(data_column)\n",
    "        if add_percentage == False:\n",
    "            percentage = p.get_height()\n",
    "        else:\n",
    "            percentage = '{:.1f}%'.format(100 * p.get_height()/total) + '\\n(' + str(p.get_height()) + ')'\n",
    "        x = p.get_x() + p.get_width() / 2 \n",
    "        y = p.get_y() + p.get_height() + 0.1\n",
    "        chart.annotate(percentage, (x, y), size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438e724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 33\n",
    "df_on_rec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6c69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 34\n",
    "# We graph the recommended instance distribution\n",
    "# This will be useful to understand the final costs of EC2, when comparing different scenarios\n",
    "chart_category(df_on_rec['Recommended_EC2_Instance'], 20, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbce4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 35\n",
    "# Graphing other interesting columns such as OS type and windows versions\n",
    "interesting_columns = ['OS_Type', 'EBS_Type']\n",
    "for column in interesting_columns:\n",
    "    chart_category(df_on_rec[column], 10, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b12e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 36\n",
    "# We will analyse the Windows version distribution\n",
    "df_on_rec_win = df_on_rec[df_on_rec['OS_Type']=='Windows']\n",
    "chart_category(df_on_rec_win['Operating_System'], 20, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a345c4",
   "metadata": {},
   "source": [
    "## VMs with 0 disk IOPs and with < 10 disk IOPs\n",
    "\n",
    "In this section we provide the list of VMs that didn't show disk IOPs activity during ADS collection. Those VMs could be removed from the migration project to save costs in AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85aa12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 37\n",
    "# To have consistency with Migration Evaluator, we will call the VMs with 0 disk IOPs as zombie VMs\n",
    "df_zombie = df_on_rec[df_on_rec[\"Iops_total_(R+W)-Max\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7ae76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 38\n",
    "# We save Zombies VMs in a CSV\n",
    "df_zombie.to_csv('Zombie_vm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd5a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 39\n",
    "# To have consistency with Migration Evaluator, we will call the VMs with 10 disk IOPs or less, as zombie VMs\n",
    "df_zombie_ten_iops = df_on_rec[df_on_rec[\"Iops_total_(R+W)-Max\"] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eda737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 40\n",
    "# We save Zombies VMs in a CSV\n",
    "df_zombie_ten_iops.to_csv('Zombie_vm_less_than_10_iops.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff6a7d",
   "metadata": {},
   "source": [
    "## Top 10 instances, by cost\n",
    "We provide the Top 10 of the insances, by cost. We provide this list for further analysis, like for example, selecting a different OS or instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3962c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 41\n",
    "ds_top10 = df_on_rec.sort_values('EC2_Monthly_Cost', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17050faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 42\n",
    "ds_top10.to_csv('top_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b882610",
   "metadata": {},
   "source": [
    "# 4. Cost Analysis and Recommendations Export\n",
    "\n",
    "At the end, we are interested on what is going to be the infrastructure cost on AWS (EC2, EBS and DT0). In this section we provide a summary of those coses.\n",
    "It's recommended to evaluate different scenarios, such as Direct Match, and 95 Percentile, to determine which one is more convenient for the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af015819",
   "metadata": {},
   "source": [
    "## EC2 Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aae283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 43\n",
    "ec2_cost = df_on_rec['EC2_Monthly_Cost'].sum().round(2)\n",
    "print(f\"The total EC2 Monthly Cost would be $ {ec2_cost} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603325dc",
   "metadata": {},
   "source": [
    "## EBS Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cad19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 44\n",
    "ebs_cost = df_on_rec['EBS_Monthly_Cost'].sum().round(2)\n",
    "print(f\"The total EBS Monthly Cost would be $ {ebs_cost} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e095b9a",
   "metadata": {},
   "source": [
    "## DTO Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02553b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 45\n",
    "total_gb_dto = df_on_rec['Data_Transfer_Out-GiB_written'].sum().round(2)*0.2\n",
    "\n",
    "dto_monthly_cost = 0\n",
    "if total_gb_dto < 10*1024:\n",
    "    dto_monthly_cost = total_gb_dto * cost_10_tb\n",
    "elif total_gb_dto < 50*1024:\n",
    "    dto_monthly_cost = 10*1024*cost_10_tb + (total_gb_dto - 10*1024)*cost_40_tb\n",
    "elif total_gb_dto < 150*1024:\n",
    "    dto_monthly_cost = 10*1024*cost_10_tb + 40*1024*cost_40_tb + (total_gb_dto - 50*1024)*cost_100_tb\n",
    "else:\n",
    "    dto_monthly_cost = 10*1024*cost_10_tb + 40*1024*cost_40_tb + 100*1024*cost_100_tb + (total_gb_dto - 150*1024)*cost_150_tb\n",
    "    \n",
    "print(f\"The total DTO Monthly Cost would be $ {dto_monthly_cost.round(2)} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7781290",
   "metadata": {},
   "source": [
    "## Total Cost of Ownership (TCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417b334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 46\n",
    "print(f\"The total estimated TCO is $ {ec2_cost + ebs_cost + dto_monthly_cost.round(2)} USD (Monthly)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80de301",
   "metadata": {},
   "source": [
    "## Export recommendation file\n",
    "\n",
    "Finally, we export the recommendation file with the recommended instances and storage per VM. This is useful to do a bulk import into the AWS Calculator https://calculator.aws/ and try different purchase options, such as Compute Savings Plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed885897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 47\n",
    "# Exporting the recommendation file\n",
    "df_on_rec.to_csv(ec2_recommendation_file.split('.')[0] + \"_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923351df",
   "metadata": {},
   "source": [
    "# ------------ END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
